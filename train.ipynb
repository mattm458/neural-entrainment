{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mmcneil/src/neural-entrainment/venv/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from os import path\n",
    "from random import random\n",
    "\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torchaudio\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from dataset import WindowedFeatureDataset\n",
    "from model.entrainment import FixedHistoryEntrainmentModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tail_length = -1\n",
    "history = \"self\"\n",
    "attention = False\n",
    "corpus_dir = \"/home/mmcneil/datasets/fisher_corpus\"\n",
    "epochs = 10\n",
    "batch_size = 128\n",
    "input_features = [\n",
    "    \"intensity_mean_norm\",\n",
    "    \"pitch_range_norm\",\n",
    "    \"rate_norm\",\n",
    "    \"duration_norm\",\n",
    "]\n",
    "output_features = [\"pitch_range_norm\"]  # input_features\n",
    "feature_dir = \"data/fisher\"\n",
    "feature_idx = dict((v, k) for (k, v) in enumerate(input_features))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_turns = pd.read_csv(path.join(feature_dir, \"turns_norm.csv\"))\n",
    "df_turns = df_turns.set_index(\"ses_id\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureDataset(Dataset):\n",
    "    def __init__(self, df, idxs, X_features, target):\n",
    "        super().__init__()\n",
    "\n",
    "        self.df = df\n",
    "        self.idxs = idxs\n",
    "\n",
    "        self.X_features = X_features\n",
    "        self.target = target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idxs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ses_id = self.idxs[idx]\n",
    "        X_data = self.df.loc[ses_id]\n",
    "        X_features = torch.FloatTensor(X_data[self.X_features].values)\n",
    "        y_features = torch.FloatTensor(X_data[self.target].values)\n",
    "        speaker = torch.FloatTensor(\n",
    "            [[1, 0] if x == \"A\" else [0, 1] for x in X_data[\"speaker\"].values]\n",
    "        )\n",
    "\n",
    "        X_features = X_features[:-1]\n",
    "        X_speaker = speaker[:-1]\n",
    "        y_features = y_features[1:]\n",
    "        y_speaker = speaker[1:]\n",
    "\n",
    "        return (\n",
    "            X_features,\n",
    "            X_speaker,\n",
    "            torch.LongTensor([len(X_features)]),\n",
    "            y_features,\n",
    "            y_speaker,\n",
    "            torch.LongTensor([len(y_features)]),\n",
    "        )\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    X_features = nn.utils.rnn.pad_sequence([x[0] for x in batch], batch_first=True)\n",
    "    X_speaker = nn.utils.rnn.pad_sequence([x[1] for x in batch], batch_first=True)\n",
    "    X_len = torch.LongTensor([x[2] for x in batch])\n",
    "    y_features = nn.utils.rnn.pad_sequence([x[3] for x in batch], batch_first=True)\n",
    "    y_speaker = nn.utils.rnn.pad_sequence([x[4] for x in batch], batch_first=True)\n",
    "    y_len = torch.LongTensor([x[5] for x in batch])\n",
    "\n",
    "    return X_features, X_speaker, X_len, y_features, y_speaker, y_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs, test_idx = train_test_split(\n",
    "    list(df_turns.index.unique()), train_size=0.8, random_state=9001\n",
    ")\n",
    "test_idx, val_idx = train_test_split(test_idx, test_size=0.5, random_state=9001)\n",
    "\n",
    "train_dataset = FeatureDataset(\n",
    "    df_turns,\n",
    "    idxs,\n",
    "    input_features,\n",
    "    output_features,\n",
    ")\n",
    "test_dataset = FeatureDataset(\n",
    "    df_turns,\n",
    "    test_idx,\n",
    "    input_features,\n",
    "    output_features,\n",
    ")\n",
    "\n",
    "val_dataset = FeatureDataset(\n",
    "    df_turns,\n",
    "    val_idx,\n",
    "    input_features,\n",
    "    output_features,\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=16,\n",
    "    persistent_workers=True,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=128,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=16,\n",
    "    persistent_workers=True,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=128,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=16,\n",
    "    persistent_workers=True,\n",
    "    collate_fn=collate_fn,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntrainmentModel(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        lr=0.0001,\n",
    "        rnn_in_dim=3,\n",
    "        rnn_out_dim=64,\n",
    "        out_dim=1,\n",
    "        has_attention=False,\n",
    "        teacher_forcing=0.5,\n",
    "        max_length=512,\n",
    "        outputs=None,\n",
    "        dropout=0.5,\n",
    "        att_dim=128,\n",
    "        feature_idx=feature_idx,\n",
    "        teacher_forcing_schedule=False,\n",
    "        teacher_forcing_end=0.5,\n",
    "        teacher_forcing_schedule_start_epochs=10,\n",
    "        teacher_forcing_schedule_transition_epochs=10,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lr = lr\n",
    "        self.teacher_forcing = teacher_forcing\n",
    "        self.teacher_forcing_start = teacher_forcing\n",
    "        self.teacher_forcing_schedule = teacher_forcing_schedule\n",
    "        self.teacher_forcing_end = teacher_forcing_end\n",
    "        self.teacher_forcing_schedule_start_epochs = (\n",
    "            teacher_forcing_schedule_start_epochs\n",
    "        )\n",
    "        self.teacher_forcing_schedule_transition_epochs = (\n",
    "            teacher_forcing_schedule_transition_epochs\n",
    "        )\n",
    "        self.max_length = max_length\n",
    "        self.rnn_out_dim = rnn_out_dim\n",
    "\n",
    "        if has_attention:\n",
    "            raise NotImplementedError(\"Attention not implemented\")\n",
    "\n",
    "        self.att_w1 = nn.Linear(rnn_out_dim, att_dim, bias=False)\n",
    "        self.att_w2 = nn.Linear(rnn_out_dim, att_dim, bias=False)\n",
    "        self.att_v = nn.Linear(att_dim, 1, bias=False)\n",
    "\n",
    "        self.hidden_dim = rnn_out_dim\n",
    "\n",
    "        self.prenet = nn.Linear(rnn_in_dim + 4, rnn_out_dim)\n",
    "        self.rnn1 = nn.LSTMCell(rnn_out_dim, rnn_out_dim)\n",
    "        self.rnn2 = nn.LSTMCell(rnn_out_dim, rnn_out_dim)\n",
    "\n",
    "        self.decoder_rnn_1 = nn.LSTMCell(rnn_out_dim, rnn_out_dim)\n",
    "        self.decoder_rnn_2 = nn.LSTMCell(rnn_out_dim, rnn_out_dim)\n",
    "\n",
    "        self.linear = nn.Linear(rnn_out_dim * 2, out_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.outputs = outputs\n",
    "        if outputs is not None:\n",
    "            self.output_idxs = torch.LongTensor(\n",
    "                [feature_idx[output] for output in outputs]\n",
    "            )\n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "        self.log(\"teacher_forcing\", self.teacher_forcing, on_epoch=True)\n",
    "\n",
    "        teacher_forcing_epoch = (\n",
    "            self.current_epoch - self.teacher_forcing_schedule_start_epochs\n",
    "        )\n",
    "\n",
    "        if (\n",
    "            teacher_forcing_epoch > 0\n",
    "            and teacher_forcing_epoch < self.teacher_forcing_schedule_transition_epochs\n",
    "        ):\n",
    "            diff = self.teacher_forcing_start - self.teacher_forcing_end\n",
    "            diff /= self.teacher_forcing_schedule_transition_epochs\n",
    "            self.teacher_forcing -= diff\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        X_features, X_speaker, X_len, y_features, y_speaker, y_len = batch\n",
    "        y_pred, att = self(X_features, X_len, X_speaker, y_speaker, teacher_forcing=0.0)\n",
    "        loss = F.smooth_l1_loss(y_pred, y_features)\n",
    "\n",
    "        self.log(\"val_loss\", loss.detach(), on_epoch=True, on_step=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        X_features, X_speaker, X_len, y_features, y_speaker, y_len = batch\n",
    "        y_pred, att = self(X_features, X_len, X_speaker, y_speaker, teacher_forcing=0.0)\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        X_features, X_speaker, X_len, y_features, y_speaker, y_len = batch\n",
    "        y_pred, att = self(\n",
    "            X_features,\n",
    "            X_len,\n",
    "            X_speaker,\n",
    "            y_speaker,\n",
    "            teacher_forcing=self.teacher_forcing,\n",
    "        )\n",
    "\n",
    "        loss = F.mse_loss(y_pred, y_features)\n",
    "\n",
    "        self.log(\"train_loss\", loss.detach(), on_epoch=True, on_step=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def forward(self, X_features, X_len, X_speaker, y_speaker, teacher_forcing=0.5):\n",
    "        batch_size = X_features.shape[0]\n",
    "        max_len = X_len.max()\n",
    "\n",
    "        mask = torch.arange(max_len, device=self.device).unsqueeze(0).repeat(\n",
    "            len(X_len), 1\n",
    "        ) >= X_len.unsqueeze(1)\n",
    "        mask = mask.unsqueeze(2)\n",
    "\n",
    "        h1 = torch.zeros(batch_size, self.hidden_dim, device=self.device)\n",
    "        c1 = torch.zeros(batch_size, self.hidden_dim, device=self.device)\n",
    "        h2 = torch.zeros(batch_size, self.hidden_dim, device=self.device)\n",
    "        c2 = torch.zeros(batch_size, self.hidden_dim, device=self.device)\n",
    "\n",
    "        decoder_h_1 = torch.zeros(batch_size, self.hidden_dim, device=self.device)\n",
    "        decoder_c_1 = torch.zeros(batch_size, self.hidden_dim, device=self.device)\n",
    "        decoder_h_2 = torch.zeros(batch_size, self.hidden_dim, device=self.device)\n",
    "        decoder_c_2 = torch.zeros(batch_size, self.hidden_dim, device=self.device)\n",
    "\n",
    "        out = []\n",
    "        rnn_out = torch.zeros(batch_size, max_len, self.rnn_out_dim, device=self.device)\n",
    "\n",
    "        prev_out = None\n",
    "        all_weights = []\n",
    "        for i in range(max_len):\n",
    "            if prev_out is not None and random() > teacher_forcing:\n",
    "                if self.outputs is None:\n",
    "                    input = prev_out\n",
    "                else:\n",
    "                    input = X_features[:, i]\n",
    "                    input[:, self.output_idxs] = prev_out.type_as(input)\n",
    "            else:\n",
    "                input = X_features[:, i]\n",
    "\n",
    "            input = torch.cat([input, X_speaker[:, i], y_speaker[:, i]], dim=-1)\n",
    "\n",
    "            input = F.relu(self.prenet(input))\n",
    "\n",
    "            h1, c1 = self.rnn1(input, (h1, c1))\n",
    "            h1 = self.dropout(h1)\n",
    "            h2, c2 = self.rnn2(h1, (h2, c2))\n",
    "            h2 = self.dropout(h2)\n",
    "\n",
    "            rnn_out[:, i] = h2\n",
    "\n",
    "            decoder_h_1, decoder_c_1 = self.decoder_rnn_1(\n",
    "                h2, (decoder_h_1, decoder_c_1)\n",
    "            )\n",
    "            decoder_h_1 = self.dropout(decoder_h_1)\n",
    "            decoder_h_2, decoder_c_2 = self.decoder_rnn_2(\n",
    "                decoder_h_1, (decoder_h_2, decoder_c_2)\n",
    "            )\n",
    "            decoder_h_2 = self.dropout(decoder_h_2)\n",
    "\n",
    "            w1 = self.att_w1(rnn_out[:, : i + 1])\n",
    "            w2 = self.att_w2(decoder_h_2).unsqueeze(1)\n",
    "            score = self.att_v(torch.tanh(w1 + w2))\n",
    "\n",
    "            score = score.masked_fill(mask[:, : i + 1], float(\"-inf\"))\n",
    "            score = F.softmax(score, dim=1)\n",
    "            all_weights.append(score.detach().unsqueeze(1))\n",
    "            score = score.squeeze(-1)\n",
    "\n",
    "            score = score.unsqueeze(1)\n",
    "            att_applied = torch.bmm(score, rnn_out[:, : i + 1])\n",
    "            att_applied = att_applied.squeeze(1)\n",
    "\n",
    "            h_out = self.linear(torch.cat([decoder_h_2, att_applied], dim=1))\n",
    "            out.append(h_out.unsqueeze(1))\n",
    "            prev_out = h_out.detach()\n",
    "\n",
    "        out = torch.cat(out, dim=1)\n",
    "        out = out.masked_fill(mask, 0)\n",
    "\n",
    "        return out, all_weights\n",
    "\n",
    "\n",
    "model = EntrainmentModel(\n",
    "    lr=0.0001,\n",
    "    rnn_in_dim=len(input_features),\n",
    "    rnn_out_dim=32,\n",
    "    out_dim=1,  # len(output_features),\n",
    "    has_attention=False,\n",
    "    teacher_forcing=0.0,\n",
    "    dropout=0.0,\n",
    "    outputs=[\"pitch_range_norm\"],\n",
    ")\n",
    "\n",
    "for x in test_dataloader:\n",
    "    model.training_step(x, 0)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit native Automatic Mixed Precision (AMP)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name          | Type     | Params\n",
      "-------------------------------------------\n",
      "0 | att_w1        | Linear   | 4.1 K \n",
      "1 | att_w2        | Linear   | 4.1 K \n",
      "2 | att_v         | Linear   | 128   \n",
      "3 | prenet        | Linear   | 288   \n",
      "4 | rnn1          | LSTMCell | 8.4 K \n",
      "5 | rnn2          | LSTMCell | 8.4 K \n",
      "6 | decoder_rnn_1 | LSTMCell | 8.4 K \n",
      "7 | decoder_rnn_2 | LSTMCell | 8.4 K \n",
      "8 | linear        | Linear   | 65    \n",
      "9 | dropout       | Dropout  | 0     \n",
      "-------------------------------------------\n",
      "42.5 K    Trainable params\n",
      "0         Non-trainable params\n",
      "42.5 K    Total params\n",
      "0.085     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16:  46%|████▌     | 38/83 [02:08<02:32,  3.38s/it, loss=0.552, v_num=145]\n",
      "Epoch 14:  10%|▉         | 8/83 [00:04<00:38,  1.93it/s, loss=0.545, v_num=146] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mmcneil/src/neural-entrainment/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:726: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "model = EntrainmentModel(\n",
    "    lr=0.00001,\n",
    "    rnn_in_dim=len(input_features),\n",
    "    rnn_out_dim=32,\n",
    "    att_dim=128,\n",
    "    out_dim=1,  # len(output_features),\n",
    "    has_attention=False,\n",
    "    teacher_forcing=1.0,\n",
    "    dropout=0.0,\n",
    "    outputs=[\"pitch_range_norm\"],\n",
    "    teacher_forcing_schedule=True,\n",
    "    teacher_forcing_end=0.5,\n",
    "    teacher_forcing_schedule_start_epochs=10,\n",
    "    teacher_forcing_schedule_transition_epochs=10,\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(accelerator=\"gpu\", devices=[0], precision=16, max_epochs=-1)\n",
    "trainer.fit(model, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49 1.0\n",
      "49 1.0\n",
      "49 1.0\n",
      "49 1.0\n",
      "49 1.0\n",
      "49 1.0\n",
      "49 1.0\n",
      "49 1.0\n",
      "49 1.0\n",
      "49 1.0\n",
      "49 1.0\n",
      "49 0.95\n",
      "49 0.8999999999999999\n",
      "49 0.8499999999999999\n",
      "49 0.7999999999999998\n",
      "49 0.7499999999999998\n",
      "49 0.6999999999999997\n",
      "49 0.6499999999999997\n",
      "49 0.5999999999999996\n",
      "49 0.5499999999999996\n",
      "49 0.4999999999999996\n",
      "49 0.4999999999999996\n",
      "49 0.4999999999999996\n",
      "49 0.4999999999999996\n",
      "49 0.4999999999999996\n",
      "49 0.4999999999999996\n",
      "49 0.4999999999999996\n",
      "49 0.4999999999999996\n",
      "49 0.4999999999999996\n",
      "49 0.4999999999999996\n",
      "49 0.4999999999999996\n",
      "49 0.4999999999999996\n",
      "49 0.4999999999999996\n",
      "49 0.4999999999999996\n",
      "49 0.4999999999999996\n",
      "49 0.4999999999999996\n",
      "49 0.4999999999999996\n",
      "49 0.4999999999999996\n",
      "49 0.4999999999999996\n",
      "49 0.4999999999999996\n",
      "49 0.4999999999999996\n",
      "49 0.4999999999999996\n",
      "49 0.4999999999999996\n",
      "49 0.4999999999999996\n",
      "49 0.4999999999999996\n",
      "49 0.4999999999999996\n",
      "49 0.4999999999999996\n",
      "49 0.4999999999999996\n",
      "49 0.4999999999999996\n",
      "49 0.4999999999999996\n"
     ]
    }
   ],
   "source": [
    "teacher_forcing=1.0\n",
    "teacher_forcing_end=0.5\n",
    "teacher_forcing_schedule_start_epochs=10\n",
    "teacher_forcing_schedule_transition_epochs=10\n",
    "\n",
    "teacher_forcing_start = teacher_forcing\n",
    "\n",
    "for current_epoch in range(50):\n",
    "    teacher_forcing_epoch = (\n",
    "        current_epoch - teacher_forcing_schedule_start_epochs\n",
    "    )\n",
    "\n",
    "    if (\n",
    "        teacher_forcing_epoch > 0\n",
    "        and teacher_forcing_epoch <= teacher_forcing_schedule_transition_epochs\n",
    "    ):\n",
    "        diff = teacher_forcing_start - teacher_forcing_end\n",
    "        diff /= teacher_forcing_schedule_transition_epochs\n",
    "        teacher_forcing -= diff\n",
    "\n",
    "    print(i, teacher_forcing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for EntrainmentModel:\n\tUnexpected key(s) in state_dict: \"att_w1.bias\", \"att_w2.bias\", \"att_v.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/mmcneil/src/neural-entrainment/train.ipynb Cell 9'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/mmcneil/src/neural-entrainment/train.ipynb#ch0000015?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m EntrainmentModel\u001b[39m.\u001b[39;49mload_from_checkpoint(\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/mmcneil/src/neural-entrainment/train.ipynb#ch0000015?line=1'>2</a>\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mlightning_logs/version_140/checkpoints/epoch=55-step=4088.ckpt\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/mmcneil/src/neural-entrainment/train.ipynb#ch0000015?line=2'>3</a>\u001b[0m     lr\u001b[39m=\u001b[39;49m\u001b[39m0.00001\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/mmcneil/src/neural-entrainment/train.ipynb#ch0000015?line=3'>4</a>\u001b[0m     rnn_in_dim\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(input_features),\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/mmcneil/src/neural-entrainment/train.ipynb#ch0000015?line=4'>5</a>\u001b[0m     rnn_out_dim\u001b[39m=\u001b[39;49m\u001b[39m128\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/mmcneil/src/neural-entrainment/train.ipynb#ch0000015?line=5'>6</a>\u001b[0m     out_dim\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,  \u001b[39m# len(output_features),\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/mmcneil/src/neural-entrainment/train.ipynb#ch0000015?line=6'>7</a>\u001b[0m     has_attention\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/mmcneil/src/neural-entrainment/train.ipynb#ch0000015?line=7'>8</a>\u001b[0m     teacher_forcing\u001b[39m=\u001b[39;49m\u001b[39m0.5\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/mmcneil/src/neural-entrainment/train.ipynb#ch0000015?line=8'>9</a>\u001b[0m     dropout\u001b[39m=\u001b[39;49m\u001b[39m0.5\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mmcneil/src/neural-entrainment/train.ipynb#ch0000015?line=9'>10</a>\u001b[0m     outputs\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39mpitch_range_norm\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mmcneil/src/neural-entrainment/train.ipynb#ch0000015?line=10'>11</a>\u001b[0m )\n",
      "File \u001b[0;32m~/src/neural-entrainment/venv/lib/python3.9/site-packages/pytorch_lightning/core/saving.py:161\u001b[0m, in \u001b[0;36mModelIO.load_from_checkpoint\u001b[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/mmcneil//src/neural-entrainment/venv/lib/python3.9/site-packages/pytorch_lightning/core/saving.py?line=157'>158</a>\u001b[0m \u001b[39m# override the hparams with values that were passed in\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/mmcneil//src/neural-entrainment/venv/lib/python3.9/site-packages/pytorch_lightning/core/saving.py?line=158'>159</a>\u001b[0m checkpoint[\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mCHECKPOINT_HYPER_PARAMS_KEY]\u001b[39m.\u001b[39mupdate(kwargs)\n\u001b[0;32m--> <a href='file:///home/mmcneil//src/neural-entrainment/venv/lib/python3.9/site-packages/pytorch_lightning/core/saving.py?line=160'>161</a>\u001b[0m model \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_load_model_state(checkpoint, strict\u001b[39m=\u001b[39;49mstrict, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/mmcneil//src/neural-entrainment/venv/lib/python3.9/site-packages/pytorch_lightning/core/saving.py?line=161'>162</a>\u001b[0m \u001b[39mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/src/neural-entrainment/venv/lib/python3.9/site-packages/pytorch_lightning/core/saving.py:209\u001b[0m, in \u001b[0;36mModelIO._load_model_state\u001b[0;34m(cls, checkpoint, strict, **cls_kwargs_new)\u001b[0m\n\u001b[1;32m    <a href='file:///home/mmcneil//src/neural-entrainment/venv/lib/python3.9/site-packages/pytorch_lightning/core/saving.py?line=205'>206</a>\u001b[0m model\u001b[39m.\u001b[39mon_load_checkpoint(checkpoint)\n\u001b[1;32m    <a href='file:///home/mmcneil//src/neural-entrainment/venv/lib/python3.9/site-packages/pytorch_lightning/core/saving.py?line=207'>208</a>\u001b[0m \u001b[39m# load the state_dict on the model automatically\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/mmcneil//src/neural-entrainment/venv/lib/python3.9/site-packages/pytorch_lightning/core/saving.py?line=208'>209</a>\u001b[0m keys \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mload_state_dict(checkpoint[\u001b[39m\"\u001b[39;49m\u001b[39mstate_dict\u001b[39;49m\u001b[39m\"\u001b[39;49m], strict\u001b[39m=\u001b[39;49mstrict)\n\u001b[1;32m    <a href='file:///home/mmcneil//src/neural-entrainment/venv/lib/python3.9/site-packages/pytorch_lightning/core/saving.py?line=210'>211</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m strict:\n\u001b[1;32m    <a href='file:///home/mmcneil//src/neural-entrainment/venv/lib/python3.9/site-packages/pytorch_lightning/core/saving.py?line=211'>212</a>\u001b[0m     \u001b[39mif\u001b[39;00m keys\u001b[39m.\u001b[39mmissing_keys:\n",
      "File \u001b[0;32m~/src/neural-entrainment/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1497\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   <a href='file:///home/mmcneil//src/neural-entrainment/venv/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1491'>1492</a>\u001b[0m         error_msgs\u001b[39m.\u001b[39minsert(\n\u001b[1;32m   <a href='file:///home/mmcneil//src/neural-entrainment/venv/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1492'>1493</a>\u001b[0m             \u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMissing key(s) in state_dict: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   <a href='file:///home/mmcneil//src/neural-entrainment/venv/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1493'>1494</a>\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(k) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m missing_keys)))\n\u001b[1;32m   <a href='file:///home/mmcneil//src/neural-entrainment/venv/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1495'>1496</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(error_msgs) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m-> <a href='file:///home/mmcneil//src/neural-entrainment/venv/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1496'>1497</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mError(s) in loading state_dict for \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   <a href='file:///home/mmcneil//src/neural-entrainment/venv/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1497'>1498</a>\u001b[0m                        \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   <a href='file:///home/mmcneil//src/neural-entrainment/venv/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1498'>1499</a>\u001b[0m \u001b[39mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for EntrainmentModel:\n\tUnexpected key(s) in state_dict: \"att_w1.bias\", \"att_w2.bias\", \"att_v.bias\". "
     ]
    }
   ],
   "source": [
    "model = EntrainmentModel.load_from_checkpoint(\n",
    "    \"lightning_logs/version_140/checkpoints/epoch=55-step=4088.ckpt\",\n",
    "    lr=0.00001,\n",
    "    rnn_in_dim=len(input_features),\n",
    "    rnn_out_dim=128,\n",
    "    out_dim=1,  # len(output_features),\n",
    "    has_attention=False,\n",
    "    teacher_forcing=0.5,\n",
    "    dropout=0.5,\n",
    "    outputs=[\"pitch_range_norm\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cuda()\n",
    "for batch in test_dataloader:\n",
    "    X_features, X_speaker, X_len, y_features, y_speaker, y_len = batch\n",
    "    y_pred, att = model(\n",
    "        X_features.cuda(),\n",
    "        X_len.cuda(),\n",
    "        X_speaker.cuda(),\n",
    "        y_speaker.cuda(),\n",
    "        teacher_forcing=0.0,\n",
    "    )\n",
    "\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "att_together = nn.utils.rnn.pad_sequence(\n",
    "    [x.swapaxes(0, 3).squeeze(0).squeeze(0) for x in att]\n",
    ").swapaxes(0, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.3247],\n",
       "          [0.3345],\n",
       "          [0.3407]]],\n",
       "\n",
       "\n",
       "        [[[0.3250],\n",
       "          [0.3343],\n",
       "          [0.3407]]],\n",
       "\n",
       "\n",
       "        [[[0.3245],\n",
       "          [0.3346],\n",
       "          [0.3409]]],\n",
       "\n",
       "\n",
       "        [[[0.3247],\n",
       "          [0.3344],\n",
       "          [0.3409]]],\n",
       "\n",
       "\n",
       "        [[[0.3247],\n",
       "          [0.3345],\n",
       "          [0.3408]]],\n",
       "\n",
       "\n",
       "        [[[0.3243],\n",
       "          [0.3348],\n",
       "          [0.3409]]],\n",
       "\n",
       "\n",
       "        [[[0.3239],\n",
       "          [0.3348],\n",
       "          [0.3413]]],\n",
       "\n",
       "\n",
       "        [[[0.3249],\n",
       "          [0.3346],\n",
       "          [0.3405]]],\n",
       "\n",
       "\n",
       "        [[[0.3245],\n",
       "          [0.3347],\n",
       "          [0.3408]]],\n",
       "\n",
       "\n",
       "        [[[0.3246],\n",
       "          [0.3345],\n",
       "          [0.3409]]],\n",
       "\n",
       "\n",
       "        [[[0.3250],\n",
       "          [0.3342],\n",
       "          [0.3408]]],\n",
       "\n",
       "\n",
       "        [[[0.3251],\n",
       "          [0.3346],\n",
       "          [0.3403]]],\n",
       "\n",
       "\n",
       "        [[[0.3248],\n",
       "          [0.3345],\n",
       "          [0.3407]]],\n",
       "\n",
       "\n",
       "        [[[0.3248],\n",
       "          [0.3341],\n",
       "          [0.3411]]],\n",
       "\n",
       "\n",
       "        [[[0.3243],\n",
       "          [0.3347],\n",
       "          [0.3410]]],\n",
       "\n",
       "\n",
       "        [[[0.3242],\n",
       "          [0.3348],\n",
       "          [0.3410]]],\n",
       "\n",
       "\n",
       "        [[[0.3242],\n",
       "          [0.3347],\n",
       "          [0.3411]]],\n",
       "\n",
       "\n",
       "        [[[0.3245],\n",
       "          [0.3342],\n",
       "          [0.3413]]],\n",
       "\n",
       "\n",
       "        [[[0.3247],\n",
       "          [0.3344],\n",
       "          [0.3409]]],\n",
       "\n",
       "\n",
       "        [[[0.3248],\n",
       "          [0.3344],\n",
       "          [0.3408]]],\n",
       "\n",
       "\n",
       "        [[[0.3246],\n",
       "          [0.3343],\n",
       "          [0.3411]]],\n",
       "\n",
       "\n",
       "        [[[0.3250],\n",
       "          [0.3348],\n",
       "          [0.3402]]],\n",
       "\n",
       "\n",
       "        [[[0.3243],\n",
       "          [0.3345],\n",
       "          [0.3411]]],\n",
       "\n",
       "\n",
       "        [[[0.3244],\n",
       "          [0.3348],\n",
       "          [0.3408]]],\n",
       "\n",
       "\n",
       "        [[[0.3249],\n",
       "          [0.3342],\n",
       "          [0.3408]]],\n",
       "\n",
       "\n",
       "        [[[0.3248],\n",
       "          [0.3346],\n",
       "          [0.3406]]],\n",
       "\n",
       "\n",
       "        [[[0.3248],\n",
       "          [0.3342],\n",
       "          [0.3410]]],\n",
       "\n",
       "\n",
       "        [[[0.3248],\n",
       "          [0.3346],\n",
       "          [0.3406]]],\n",
       "\n",
       "\n",
       "        [[[0.3248],\n",
       "          [0.3345],\n",
       "          [0.3407]]],\n",
       "\n",
       "\n",
       "        [[[0.3245],\n",
       "          [0.3345],\n",
       "          [0.3410]]],\n",
       "\n",
       "\n",
       "        [[[0.3248],\n",
       "          [0.3346],\n",
       "          [0.3406]]],\n",
       "\n",
       "\n",
       "        [[[0.3253],\n",
       "          [0.3344],\n",
       "          [0.3402]]],\n",
       "\n",
       "\n",
       "        [[[0.3249],\n",
       "          [0.3341],\n",
       "          [0.3409]]],\n",
       "\n",
       "\n",
       "        [[[0.3243],\n",
       "          [0.3348],\n",
       "          [0.3409]]],\n",
       "\n",
       "\n",
       "        [[[0.3246],\n",
       "          [0.3347],\n",
       "          [0.3407]]],\n",
       "\n",
       "\n",
       "        [[[0.3248],\n",
       "          [0.3344],\n",
       "          [0.3408]]],\n",
       "\n",
       "\n",
       "        [[[0.3244],\n",
       "          [0.3348],\n",
       "          [0.3408]]],\n",
       "\n",
       "\n",
       "        [[[0.3246],\n",
       "          [0.3347],\n",
       "          [0.3407]]],\n",
       "\n",
       "\n",
       "        [[[0.3251],\n",
       "          [0.3342],\n",
       "          [0.3406]]],\n",
       "\n",
       "\n",
       "        [[[0.3249],\n",
       "          [0.3345],\n",
       "          [0.3406]]],\n",
       "\n",
       "\n",
       "        [[[0.3247],\n",
       "          [0.3344],\n",
       "          [0.3409]]],\n",
       "\n",
       "\n",
       "        [[[0.3248],\n",
       "          [0.3344],\n",
       "          [0.3408]]],\n",
       "\n",
       "\n",
       "        [[[0.3248],\n",
       "          [0.3342],\n",
       "          [0.3410]]],\n",
       "\n",
       "\n",
       "        [[[0.3243],\n",
       "          [0.3348],\n",
       "          [0.3410]]],\n",
       "\n",
       "\n",
       "        [[[0.3249],\n",
       "          [0.3347],\n",
       "          [0.3404]]],\n",
       "\n",
       "\n",
       "        [[[0.3252],\n",
       "          [0.3345],\n",
       "          [0.3403]]],\n",
       "\n",
       "\n",
       "        [[[0.3247],\n",
       "          [0.3346],\n",
       "          [0.3408]]],\n",
       "\n",
       "\n",
       "        [[[0.3251],\n",
       "          [0.3345],\n",
       "          [0.3404]]],\n",
       "\n",
       "\n",
       "        [[[0.3245],\n",
       "          [0.3345],\n",
       "          [0.3411]]],\n",
       "\n",
       "\n",
       "        [[[0.3241],\n",
       "          [0.3348],\n",
       "          [0.3411]]],\n",
       "\n",
       "\n",
       "        [[[0.3248],\n",
       "          [0.3346],\n",
       "          [0.3406]]],\n",
       "\n",
       "\n",
       "        [[[0.3250],\n",
       "          [0.3345],\n",
       "          [0.3405]]],\n",
       "\n",
       "\n",
       "        [[[0.3244],\n",
       "          [0.3347],\n",
       "          [0.3409]]],\n",
       "\n",
       "\n",
       "        [[[0.3249],\n",
       "          [0.3343],\n",
       "          [0.3408]]],\n",
       "\n",
       "\n",
       "        [[[0.3249],\n",
       "          [0.3345],\n",
       "          [0.3406]]],\n",
       "\n",
       "\n",
       "        [[[0.3249],\n",
       "          [0.3344],\n",
       "          [0.3407]]],\n",
       "\n",
       "\n",
       "        [[[0.3249],\n",
       "          [0.3345],\n",
       "          [0.3406]]],\n",
       "\n",
       "\n",
       "        [[[0.3245],\n",
       "          [0.3346],\n",
       "          [0.3409]]],\n",
       "\n",
       "\n",
       "        [[[0.3246],\n",
       "          [0.3345],\n",
       "          [0.3409]]],\n",
       "\n",
       "\n",
       "        [[[0.3246],\n",
       "          [0.3345],\n",
       "          [0.3409]]],\n",
       "\n",
       "\n",
       "        [[[0.3247],\n",
       "          [0.3347],\n",
       "          [0.3406]]],\n",
       "\n",
       "\n",
       "        [[[0.3247],\n",
       "          [0.3343],\n",
       "          [0.3410]]],\n",
       "\n",
       "\n",
       "        [[[0.3248],\n",
       "          [0.3344],\n",
       "          [0.3408]]],\n",
       "\n",
       "\n",
       "        [[[0.3248],\n",
       "          [0.3345],\n",
       "          [0.3407]]],\n",
       "\n",
       "\n",
       "        [[[0.3245],\n",
       "          [0.3343],\n",
       "          [0.3413]]],\n",
       "\n",
       "\n",
       "        [[[0.3241],\n",
       "          [0.3348],\n",
       "          [0.3411]]],\n",
       "\n",
       "\n",
       "        [[[0.3247],\n",
       "          [0.3345],\n",
       "          [0.3408]]],\n",
       "\n",
       "\n",
       "        [[[0.3248],\n",
       "          [0.3345],\n",
       "          [0.3408]]],\n",
       "\n",
       "\n",
       "        [[[0.3249],\n",
       "          [0.3347],\n",
       "          [0.3404]]],\n",
       "\n",
       "\n",
       "        [[[0.3244],\n",
       "          [0.3345],\n",
       "          [0.3411]]],\n",
       "\n",
       "\n",
       "        [[[0.3247],\n",
       "          [0.3342],\n",
       "          [0.3410]]],\n",
       "\n",
       "\n",
       "        [[[0.3248],\n",
       "          [0.3345],\n",
       "          [0.3407]]],\n",
       "\n",
       "\n",
       "        [[[0.3248],\n",
       "          [0.3345],\n",
       "          [0.3407]]],\n",
       "\n",
       "\n",
       "        [[[0.3250],\n",
       "          [0.3344],\n",
       "          [0.3406]]],\n",
       "\n",
       "\n",
       "        [[[0.3247],\n",
       "          [0.3346],\n",
       "          [0.3406]]],\n",
       "\n",
       "\n",
       "        [[[0.3250],\n",
       "          [0.3349],\n",
       "          [0.3401]]],\n",
       "\n",
       "\n",
       "        [[[0.3249],\n",
       "          [0.3345],\n",
       "          [0.3406]]],\n",
       "\n",
       "\n",
       "        [[[0.3244],\n",
       "          [0.3347],\n",
       "          [0.3409]]],\n",
       "\n",
       "\n",
       "        [[[0.3246],\n",
       "          [0.3345],\n",
       "          [0.3409]]],\n",
       "\n",
       "\n",
       "        [[[0.3245],\n",
       "          [0.3348],\n",
       "          [0.3407]]],\n",
       "\n",
       "\n",
       "        [[[0.3250],\n",
       "          [0.3345],\n",
       "          [0.3405]]],\n",
       "\n",
       "\n",
       "        [[[0.3243],\n",
       "          [0.3347],\n",
       "          [0.3410]]],\n",
       "\n",
       "\n",
       "        [[[0.3248],\n",
       "          [0.3345],\n",
       "          [0.3407]]],\n",
       "\n",
       "\n",
       "        [[[0.3246],\n",
       "          [0.3345],\n",
       "          [0.3409]]],\n",
       "\n",
       "\n",
       "        [[[0.3249],\n",
       "          [0.3347],\n",
       "          [0.3405]]],\n",
       "\n",
       "\n",
       "        [[[0.3250],\n",
       "          [0.3347],\n",
       "          [0.3403]]],\n",
       "\n",
       "\n",
       "        [[[0.3248],\n",
       "          [0.3349],\n",
       "          [0.3404]]],\n",
       "\n",
       "\n",
       "        [[[0.3245],\n",
       "          [0.3348],\n",
       "          [0.3407]]],\n",
       "\n",
       "\n",
       "        [[[0.3249],\n",
       "          [0.3345],\n",
       "          [0.3406]]],\n",
       "\n",
       "\n",
       "        [[[0.3243],\n",
       "          [0.3345],\n",
       "          [0.3412]]],\n",
       "\n",
       "\n",
       "        [[[0.3250],\n",
       "          [0.3344],\n",
       "          [0.3406]]],\n",
       "\n",
       "\n",
       "        [[[0.3248],\n",
       "          [0.3345],\n",
       "          [0.3407]]],\n",
       "\n",
       "\n",
       "        [[[0.3248],\n",
       "          [0.3344],\n",
       "          [0.3408]]],\n",
       "\n",
       "\n",
       "        [[[0.3246],\n",
       "          [0.3347],\n",
       "          [0.3407]]],\n",
       "\n",
       "\n",
       "        [[[0.3250],\n",
       "          [0.3346],\n",
       "          [0.3404]]],\n",
       "\n",
       "\n",
       "        [[[0.3244],\n",
       "          [0.3344],\n",
       "          [0.3412]]],\n",
       "\n",
       "\n",
       "        [[[0.3245],\n",
       "          [0.3347],\n",
       "          [0.3407]]],\n",
       "\n",
       "\n",
       "        [[[0.3243],\n",
       "          [0.3346],\n",
       "          [0.3410]]],\n",
       "\n",
       "\n",
       "        [[[0.3249],\n",
       "          [0.3344],\n",
       "          [0.3407]]],\n",
       "\n",
       "\n",
       "        [[[0.3249],\n",
       "          [0.3345],\n",
       "          [0.3406]]],\n",
       "\n",
       "\n",
       "        [[[0.3248],\n",
       "          [0.3344],\n",
       "          [0.3407]]],\n",
       "\n",
       "\n",
       "        [[[0.3250],\n",
       "          [0.3347],\n",
       "          [0.3403]]],\n",
       "\n",
       "\n",
       "        [[[0.3244],\n",
       "          [0.3345],\n",
       "          [0.3411]]],\n",
       "\n",
       "\n",
       "        [[[0.3249],\n",
       "          [0.3343],\n",
       "          [0.3408]]],\n",
       "\n",
       "\n",
       "        [[[0.3251],\n",
       "          [0.3345],\n",
       "          [0.3404]]],\n",
       "\n",
       "\n",
       "        [[[0.3241],\n",
       "          [0.3344],\n",
       "          [0.3415]]],\n",
       "\n",
       "\n",
       "        [[[0.3248],\n",
       "          [0.3347],\n",
       "          [0.3405]]],\n",
       "\n",
       "\n",
       "        [[[0.3248],\n",
       "          [0.3348],\n",
       "          [0.3404]]],\n",
       "\n",
       "\n",
       "        [[[0.3243],\n",
       "          [0.3345],\n",
       "          [0.3412]]],\n",
       "\n",
       "\n",
       "        [[[0.3248],\n",
       "          [0.3346],\n",
       "          [0.3406]]],\n",
       "\n",
       "\n",
       "        [[[0.3247],\n",
       "          [0.3346],\n",
       "          [0.3407]]],\n",
       "\n",
       "\n",
       "        [[[0.3249],\n",
       "          [0.3342],\n",
       "          [0.3409]]],\n",
       "\n",
       "\n",
       "        [[[0.3248],\n",
       "          [0.3345],\n",
       "          [0.3407]]],\n",
       "\n",
       "\n",
       "        [[[0.3250],\n",
       "          [0.3345],\n",
       "          [0.3405]]],\n",
       "\n",
       "\n",
       "        [[[0.3250],\n",
       "          [0.3345],\n",
       "          [0.3405]]],\n",
       "\n",
       "\n",
       "        [[[0.3247],\n",
       "          [0.3344],\n",
       "          [0.3408]]],\n",
       "\n",
       "\n",
       "        [[[0.3247],\n",
       "          [0.3347],\n",
       "          [0.3406]]],\n",
       "\n",
       "\n",
       "        [[[0.3247],\n",
       "          [0.3346],\n",
       "          [0.3406]]],\n",
       "\n",
       "\n",
       "        [[[0.3247],\n",
       "          [0.3347],\n",
       "          [0.3405]]],\n",
       "\n",
       "\n",
       "        [[[0.3257],\n",
       "          [0.3349],\n",
       "          [0.3394]]],\n",
       "\n",
       "\n",
       "        [[[0.3247],\n",
       "          [0.3348],\n",
       "          [0.3405]]],\n",
       "\n",
       "\n",
       "        [[[0.3247],\n",
       "          [0.3346],\n",
       "          [0.3406]]],\n",
       "\n",
       "\n",
       "        [[[0.3245],\n",
       "          [0.3345],\n",
       "          [0.3410]]],\n",
       "\n",
       "\n",
       "        [[[0.3245],\n",
       "          [0.3344],\n",
       "          [0.3411]]],\n",
       "\n",
       "\n",
       "        [[[0.3249],\n",
       "          [0.3344],\n",
       "          [0.3407]]],\n",
       "\n",
       "\n",
       "        [[[0.3245],\n",
       "          [0.3347],\n",
       "          [0.3407]]],\n",
       "\n",
       "\n",
       "        [[[0.3246],\n",
       "          [0.3345],\n",
       "          [0.3409]]],\n",
       "\n",
       "\n",
       "        [[[0.3250],\n",
       "          [0.3344],\n",
       "          [0.3406]]]], device='cuda:0')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0058, 0.0059, 0.0060, 0.0061, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062,\n",
       "        0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062,\n",
       "        0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062,\n",
       "        0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062,\n",
       "        0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062,\n",
       "        0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062,\n",
       "        0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062,\n",
       "        0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062,\n",
       "        0.0062, 0.0062, 0.0062, 0.0063, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062,\n",
       "        0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062,\n",
       "        0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062,\n",
       "        0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062,\n",
       "        0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062,\n",
       "        0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062,\n",
       "        0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062,\n",
       "        0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062,\n",
       "        0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062,\n",
       "        0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att_together[62, 160]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = 160\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.imshow(\n",
    "    att_together[62, x].unsqueeze(0).cpu()[:,:x], interpolation=\"nearest\", aspect=\"auto\"\n",
    ")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "58a8bf29bd2fe4abff04f92752d0225a67ab7baf3e2770ee4f30079d2de42df4"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
